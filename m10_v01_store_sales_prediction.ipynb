{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0. IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import inflection\n",
    "import math\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import xgboost as xgb\n",
    "import random\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import json\n",
    "import requests\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import Image\n",
    "from scipy import stats as ss\n",
    "#from html import HTML\n",
    "from IPython.core.display import display, HTML\n",
    "from tabulate import tabulate\n",
    "from boruta import BorutaPy\n",
    "from flask import Flask, request, Response\n",
    "#from rossman.Rossman import Rossman\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parâmetros de exibição de gráficos no Notebook. Fonte: \n",
    "# (https://comunidadeds.slack.com/archives/C019G4N0CF5/p1639584486292800?thread_ts=1639583542.291900&cid=C019G4N0CF5)\n",
    "def jupyter_settings():\n",
    "    %matplotlib inline\n",
    "    %pylab inline\n",
    "    \n",
    "    plt.style.use( 'bmh' )\n",
    "    plt.rcParams['figure.figsize'] = [25, 12]\n",
    "    plt.rcParams['font.size'] = 24\n",
    "    \n",
    "    #display( HTML( '<style>.container { width:100% !important; }</style>') )\n",
    "    display(HTML(\"<style>.jp-Cell { width: 60% !important; }</style>\"))\n",
    "    pd.options.display.max_columns = None\n",
    "    pd.options.display.max_rows = None\n",
    "    pd.set_option( 'display.expand_frame_repr', False )\n",
    "    \n",
    "    sns.set()\n",
    "jupyter_settings()\n",
    "\n",
    "# calculando Cramér's V, onde 0 = sem correlação, 1 = correlação total\n",
    "def cramer_v (x,y):\n",
    "    # contigency matrix: mostra todas as combinações que ocorreram entre 02 variáveis\n",
    "    cm = pd.crosstab(x, y).values\n",
    "    n = cm.sum()\n",
    "    r, k = cm.shape\n",
    "\n",
    "    chi2 = ss.chi2_contingency(cm)[0]\n",
    "\n",
    "    # corrigindo viés da fórmula\n",
    "    chi2corr = max(0, ( chi2 - ( (k-1)*(r-1) )/(n-1) ) )\n",
    "    kcorr = k - (k-1)**2/(n-1)\n",
    "    rcorr = r - (r-1)**2/(n-1)\n",
    "\n",
    "    v = np.sqrt( (chi2corr/n)/(min(kcorr-1, rcorr-1)) )\n",
    "\n",
    "    return v\n",
    "\n",
    "def ml_error (model_name, y, yhat):\n",
    "    mae = mean_absolute_error(y, yhat)\n",
    "    mape = mean_absolute_percentage_error(y, yhat)\n",
    "    rmse = np.sqrt(mean_squared_error(y, yhat))\n",
    "\n",
    "    return pd.DataFrame( {\n",
    "        'Model Name': model_name,\n",
    "        'MAE': mae,\n",
    "        'MAPE': mape,\n",
    "        'RMSE': rmse\n",
    "    }, index=[0])\n",
    "\n",
    "\n",
    "def cross_validation(x_training, kfold, model_name, model, verbose=False):\n",
    "    mae_list = []\n",
    "    mape_list = []\n",
    "    rmse_list = []\n",
    "\n",
    "    for k in reversed(range(1, kfold+1)):\n",
    "        if verbose:\n",
    "            print('\\n KFold Number: {}'.format(k))\n",
    "\n",
    "        # start and end date for validation\n",
    "        validation_start_date = x_training['date'].max() - datetime.timedelta(days=k*6*7)\n",
    "        validation_end_date = x_training['date'].max() - datetime.timedelta(days=(k-1)*6*7)\n",
    "\n",
    "        # filtering dataset\n",
    "        training = x_training[x_training['date'] < validation_start_date]\n",
    "        validation = x_training[(x_training['date'] >= validation_start_date) & (x_training['date'] <= validation_end_date)] \n",
    "\n",
    "        # training and validation datasets\n",
    "        # training\n",
    "        xtraining = training.drop(['date', 'sales'], axis=1)\n",
    "        ytraining = training['sales']\n",
    "\n",
    "        # validation\n",
    "        xvalidation = validation.drop(['date', 'sales'], axis=1)\n",
    "        yvalidation = validation['sales']\n",
    "\n",
    "        # model\n",
    "        m = model.fit(xtraining, ytraining)\n",
    "\n",
    "        # prediction\n",
    "        yhat = m.predict(xvalidation)\n",
    "\n",
    "        # performance\n",
    "        m_result = ml_error(model_name, np.expm1(yvalidation), np.expm1(yhat))\n",
    "\n",
    "        # store performance of each kfold iteration\n",
    "        mae_list.append(m_result['MAE'])\n",
    "        mape_list.append(m_result['MAPE'])\n",
    "        rmse_list.append(m_result['RMSE'])\n",
    "\n",
    "    return pd.DataFrame({'Model Name': model_name,\n",
    "                        'MAE Cross-Validation': np.round(np.mean(mae_list), 2).astype(str) + '+/-' + np.round(np.std(mae_list), 2).astype(str),\n",
    "                        'MAPE Cross-Validation': np.round(np.mean(mape_list), 2).astype(str) + '+/-' + np.round(np.std(mape_list), 2).astype(str),\n",
    "                        'RMSE Cross-Validation': np.round(np.mean(rmse_list), 2).astype(str) + '+/-' + np.round(np.std(rmse_list), 2).astype(str)\n",
    "                        }, index=[0])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_raw = pd.read_csv('data/train.csv', low_memory=False)\n",
    "df_store_raw = pd.read_csv('data/store.csv', low_memory=False)\n",
    "\n",
    "# merge\n",
    "df_raw = pd.merge(df_sales_raw, df_store_raw, how='left', on='Store')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0. DESCRICAO DOS DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sempre é bom fazer uma cópia a cada seção p/você não sobrescrever os dados originais\n",
    "# é mais fácil recuperar os dados originais\n",
    "\n",
    "df1 = df_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_old = ['Store', 'DayOfWeek', 'Date', 'Sales', 'Customers', 'Open', 'Promo', 'StateHoliday', \n",
    "       'SchoolHoliday', 'StoreType', 'Assortment','CompetitionDistance', 'CompetitionOpenSinceMonth',\n",
    "       'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval']\n",
    "\n",
    "# transforma os nomes das colunas em snakecase\n",
    "snakecase = lambda x: inflection.underscore(x)\n",
    "cols_new = list(map (snakecase, cols_old))\n",
    "\n",
    "# rename\n",
    "df1.columns = cols_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Data Dimensions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of rows: {}'.format(df1.shape[0]))\n",
    "print('Number of colums: {}'.format(df1.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Data Types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['date'] = pd.to_datetime(df1['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Check NA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Fillout NA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# competition_distance\n",
    "# vamos substituir os NA por um valor muito maior que o máximo dessa coluna.\n",
    "# usamos como valor substituinte 200000.0\n",
    "df1['competition_distance'] = df1['competition_distance'].apply(lambda x: 200000.0 if math.isnan(x) else x)\n",
    "\n",
    "# competition_open_since_month\n",
    "df1['competition_open_since_month'] = df1.apply(lambda x: x['date'].month if math.isnan(x['competition_open_since_month']) else x['competition_open_since_month'], axis=1)\n",
    "\n",
    "# competition_open_since_year   \n",
    "df1['competition_open_since_year'] = df1.apply(lambda x: x['date'].year if math.isnan(x['competition_open_since_year']) else x['competition_open_since_year'], axis=1)\n",
    "\n",
    "# promo2_since_week\n",
    "df1['promo2_since_week'] = df1.apply(lambda x: x['date'].week if math.isnan(x['promo2_since_week']) else x['promo2_since_week'], axis=1)\n",
    "\n",
    "# promo2_since_year\n",
    "df1['promo2_since_year'] = df1.apply(lambda x: x['date'].year if math.isnan(x['promo2_since_year']) else x['promo2_since_year'], axis=1)\n",
    "\n",
    "# promo_interval\n",
    "# são os meses nos quais a loja está com a promoção ativa\n",
    "# estratégia: coletar o mês atual da loja, transformar o mês em nome e verificar se esse nome do mês está na lista de meses\n",
    "# da promoção da loja.\n",
    "\n",
    "# cria um dicionário p/converter os meses numéricos em nomes\n",
    "month_map = {1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun', 7: 'Jul', 8:'Aug', 9: 'Sept', 10: 'Oct', 11: 'Nov', 12: 'Dec'}\n",
    "\n",
    "# preenche os NA da coluna com 0, sem precisar atribuir (inplace)\n",
    "df1['promo_interval'].fillna(0, inplace=True)\n",
    "\n",
    "# cria coluna month_map a partir do mês da data atual da loja, convertendo conforme o dicionário acima\n",
    "df1['month_map'] = df1['date'].dt.month.map(month_map)\n",
    "\n",
    "# se o mês atual (month_map) está contido na lista de meses de promoção (promo_interval.split), a loja está em promoção (is_promo)\n",
    "df1['is_promo'] = df1[['promo_interval', 'month_map']].apply(lambda x: 0 if x['promo_interval'] == 0 else 1 if x['month_map'] in x['promo_interval'].split(',') else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.sample(5).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. Change Types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['competition_open_since_month'] = df1['competition_open_since_month'].astype(int)\n",
    "df1['competition_open_since_year'] = df1['competition_open_since_year'].astype(int)\n",
    "df1['promo2_since_week'] = df1['promo2_since_week'].astype(int)\n",
    "df1['promo2_since_year'] = df1['promo2_since_year'].astype(int)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7. Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separando entre variáveis numéricas e categóricas\n",
    "num_attributes = df1.select_dtypes(include=['int64', 'float64'])\n",
    "cat_attributes = df1.select_dtypes(exclude=['int64', 'float64', 'datetime64[ns]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7.1. Numerical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# central tendency - mean, median\n",
    "ct1 = pd.DataFrame(num_attributes.apply(np.mean)).T\n",
    "ct2 = pd.DataFrame(num_attributes.apply(np.median)).T\n",
    "\n",
    "# dispersion - std, min, max, range, skew, kurtosis\n",
    "d1 = pd.DataFrame(num_attributes.apply(np.std)).T\n",
    "d2 = pd.DataFrame(num_attributes.apply(min)).T\n",
    "d3 = pd.DataFrame(num_attributes.apply(max)).T\n",
    "d4 = pd.DataFrame( num_attributes.apply( lambda x: x.max() - x.min() ) ).T #range\n",
    "d5 = pd.DataFrame( num_attributes.apply( lambda x: x.skew() ) ).T\n",
    "d6 = pd.DataFrame( num_attributes.apply( lambda x: x.kurtosis() ) ).T\n",
    "\n",
    "# concatenate\n",
    "m = pd.concat([d2, d3, d4, ct1, ct2, d1, d5,d6]).T.reset_index()\n",
    "m.columns = ['attributes', 'min', 'max', 'range', 'mean', 'median', 'std', 'skew', 'kurtosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df1['sales'], kind='kde', height=7, aspect=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df1['competition_distance'], )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7.2. Categorical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_attributes.apply(lambda x: x.unique().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux1 = df1[ ( df1['state_holiday'] != '0' ) & (df1['sales'] > 0) ]\n",
    "\n",
    "plt.figure(figsize=(15, 8)) #plot dimensions\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "sns.boxplot(x='state_holiday', y='sales', data=aux1)\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "sns.boxplot(x='store_type', y='sales', data=aux1)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "sns.boxplot(x='assortment', y='sales', data=aux1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Criação de Hipóteses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pense como cada atributo de agente pode impactar o fenômeno estudado (vendas na loja)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapa Mental de Hipóteses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('img/mind_map_hypothesis.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. Hipóteses de Loja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Lojas com maior quadro de funcionários deveriam vender mais.\n",
    "\n",
    "**2.** Lojas com maior estoque deveriam vender mais.\n",
    "\n",
    "**3.** Lojas com maior porte deveriam vender mais.\n",
    "\n",
    "**4.** Lojas com menor porte deveriam vender menos.\n",
    "\n",
    "**5.** Lojas com maior sortimento deveriam vender mais.\n",
    "\n",
    "**6.** Lojas com competidores mais próximos deveriam vender menos.\n",
    "\n",
    "**7.** Lojas com competidores há mais tempo deveriam vender mais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Hipóteses de Produto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Lojas que investem mais em marketing deveriam vender mais.\n",
    "\n",
    "**2.** Lojas que expõem mais o produto na vitrine deveriam vender mais.\n",
    "\n",
    "**3.** Lojas com produtos mais baratos deveriam vender mais.\n",
    "\n",
    "**4.** Lojas com promoções ativas por mais tempo deveriam vender mais.\n",
    "\n",
    "**5.** Lojas com promoções agressivas (descontos maiores) deveriam vender mais.\n",
    "\n",
    "**6.** Lojas com mais dias de promoção deveriam vender mais.\n",
    "\n",
    "**7.** Lojas com mais promoções consecutivas deveriam vender mais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3. Hipóteses de Tempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Lojas deveriam vender menos durante os feriados escolares.\n",
    "\n",
    "**2.** Lojas deveriam vender mais no segundo semestre.\n",
    "\n",
    "**3.** Lojas deveriam vender menos aos finais de semana.\n",
    "\n",
    "**4.** Lojas abertas durante o feriado de Natal deveriam vender mais.\n",
    "\n",
    "**5.** Lojas deveriam vender mais ao longo dos anos.\n",
    "\n",
    "**6.** Lojas deveriam vender mais depois do dia 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Lista Final de Hipóteses (priorização)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(o foco são hipóteses para as quais já temos os dados necessários p/validar)\n",
    "\n",
    "**1.** Lojas com maior sortimento deveriam vender mais.\n",
    "\n",
    "**2.** Lojas com competidores mais próximos deveriam vender menos.\n",
    "\n",
    "**3.** Lojas com competidores há mais tempo deveriam vender mais.\n",
    "\n",
    "**4.** Lojas com promoções ativas por mais tempo deveriam vender mais.\n",
    "\n",
    "**6.** Lojas com mais dias de promoção deveriam vender mais.\n",
    "\n",
    "**7.** Lojas com mais promoções consecutivas deveriam vender mais.\n",
    "\n",
    "**8.** Lojas deveriam vender menos durante os feriados escolares.\n",
    "\n",
    "**9.** Lojas deveriam vender mais no segundo semestre.\n",
    "\n",
    "**10.** Lojas deveriam vender menos aos finais de semana.\n",
    "\n",
    "**11.** Lojas abertas durante o feriado de Natal deveriam vender mais.\n",
    "\n",
    "**12.** Lojas deveriam vender mais ao longo dos anos.\n",
    "\n",
    "**13.** Lojas deveriam vender mais depois do dia 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtendo as variáveis necessárias para testar as hipóteses\n",
    "\n",
    "# year\n",
    "df2['year'] = df2['date'].dt.year\n",
    "\n",
    "# month\n",
    "df2['month'] = df2['date'].dt.month\n",
    "\n",
    "# day\n",
    "df2['day'] = df2['date'].dt.day\n",
    "\n",
    "# week of year\n",
    "df2['week_of_year'] = df2['date'].dt.weekofyear\n",
    "\n",
    "# year week\n",
    "df2['year_week'] = df2['date'].dt.strftime('%Y-%W')\n",
    "\n",
    "# competition since\n",
    "df2['competition_since'] = df2.apply(lambda x: datetime.datetime( year = x['competition_open_since_year'], month = x['competition_open_since_month'], day = 1 ), axis=1)\n",
    "df2['competition_time_month'] = ( (df2['date'] - df2['competition_since'])/30 ).apply(lambda x: x.days).astype(int)\n",
    "\n",
    "# promo since\n",
    "df2['promo_since'] = df2['promo2_since_year'].astype(str) + '-' + df2['promo2_since_week'].astype(str)\n",
    "df2['promo_since'] = df2['promo_since'].apply(lambda x: datetime.datetime.strptime(x + '-1', '%Y-%W-%w') - datetime.timedelta(days=7))\n",
    "df2['promo_time_week'] = ((df2['date'] - df2['promo_since'])/7).apply(lambda x: x.days).astype(int)\n",
    "\n",
    "# assortment \n",
    "# na descrição dos dados no Kaggle explica quais são os tipos de assortment\n",
    "df2['assortment'] = df2['assortment'].apply(lambda x:'basic' if x == 'a' else 'extra' if x == 'b' else 'extended')\n",
    "\n",
    "# state holiday\n",
    "df2['state_holiday'] = df2['state_holiday'].apply(lambda x: 'public_holiday' if x == 'a' else 'easter_holiday' if x == 'b' else 'christmas' if x == 'c' else 'regular_day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2['store']==22]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0. Filtragem de Variáveis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retirar variáveis que não estarão disponíveis no momento da previsão. Levar em conta o contexto de negócio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Filtragem das Linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3[ (df3['open'] != 0) & (df3['sales'] > 0) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Seleção das Colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'open' será sempre 1 depois que dropamos as linhas com 'open' = 0\n",
    "# dropamos colunas auxiliares\n",
    "\n",
    "cols_drop = ['customers', 'open', 'promo_interval', 'month_map']\n",
    "df3 = df3.drop(cols_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0. Análise Exploratória dos Dados - EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Análise Univariada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1. Variável Resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.displot(df4['sales'], kde=False, height=7, aspect=2) # retorna um objeto FacetGrid\n",
    "fig.set_axis_labels('Sales $', 'No. of Stores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.displot(df4['sales'], kde=True, log_scale=True, height=7, aspect=2) # retorna um objeto FacetGrid\n",
    "fig.set_axis_labels('Sales $', 'No. of Stores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.displot(np.log1p(df4['sales']), kde=True, height=7, aspect=2) # retorna um objeto FacetGrid\n",
    "fig.set_axis_labels('Sales $', 'No. of Stores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2. Variável Numérica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attributes.hist(bins=25);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3. Variável Categórica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_holiday\n",
    "# qual é o feriado com maior impacto nas vendas?\n",
    "\n",
    "plt.subplot(3,2,1) # cria uma matriz com 1 linha e 2 colunas. Plota o gráfico abaixo na coluna 1\n",
    "a = df4[df4['state_holiday'] != 'regular_day']\n",
    "sns.countplot(x=a['state_holiday'])\n",
    "\n",
    "plt.subplot(3,2,2)\n",
    "# sns.kdeplot(x=df4[df4['state_holiday'] == 'public_holiday']['sales'], label='public_holiday', shade=True )\n",
    "# sns.kdeplot(x=df4[df4['state_holiday'] == 'easter_holiday']['sales'], label='easter_holiday', shade=True )\n",
    "# sns.kdeplot(x=df4[df4['state_holiday'] == 'christmas']['sales'], label='christmas', shade=True )\n",
    "\n",
    "sns.kdeplot(x=df4[df4['state_holiday'] == 'public_holiday']['sales'], hue=a['state_holiday'], shade=True, warn_singular=False)\n",
    "sns.kdeplot(x=df4[df4['state_holiday'] == 'easter_holiday']['sales'], hue=a['state_holiday'], shade=True, warn_singular=False)\n",
    "sns.kdeplot(x=df4[df4['state_holiday'] == 'christmas']['sales'], hue=a['state_holiday'], shade=True, warn_singular=False)\n",
    "\n",
    "# store_type\n",
    "plt.subplot(3,2,3)\n",
    "sns.countplot(x=df4['store_type'])\n",
    "\n",
    "plt.subplot(3,2,4)\n",
    "sns.kdeplot(x=df4[df4['store_type'] == 'a']['sales'], hue=df4['store_type'], shade=True, warn_singular=False )\n",
    "sns.kdeplot(x=df4[df4['store_type'] == 'b']['sales'], hue=df4['store_type'], shade=True, warn_singular=False )\n",
    "sns.kdeplot(x=df4[df4['store_type'] == 'c']['sales'], hue=df4['store_type'], shade=True, warn_singular=False )\n",
    "sns.kdeplot(x=df4[df4['store_type'] == 'd']['sales'], hue=df4['store_type'], shade=True, warn_singular=False )\n",
    "\n",
    "# sns.kdeplot(x=df4[df4['store_type'] == 'b']['sales'], label='b', shade=True )\n",
    "# sns.kdeplot(x=df4[df4['store_type'] == 'c']['sales'], label='c', shade=True )\n",
    "# sns.kdeplot(x=df4[df4['store_type'] == 'd']['sales'], label='d', shade=True )\n",
    "\n",
    "# assortment\n",
    "plt.subplot(3,2,5)\n",
    "sns.countplot(x=df4['assortment'])\n",
    "\n",
    "plt.subplot(3,2,6   )\n",
    "sns.kdeplot(x=df4[df4['assortment'] == 'extended']['sales'], hue=df4['assortment'], shade=True, warn_singular=False )\n",
    "sns.kdeplot(x=df4[df4['assortment'] == 'basic']['sales'], hue=df4['assortment'], shade=True, warn_singular=False )\n",
    "sns.kdeplot(x=df4[df4['assortment'] == 'extra']['sales'], hue=df4['assortment'], shade=True, warn_singular=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Análise Bivariada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Lojas com maior sortimento deveriam vender mais.\n",
    "**Resposta**: FALSA. Lojas com MAIOR sortimento vendem MENOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux1 = df4[['assortment', 'sales']].groupby('assortment').sum().reset_index()\n",
    "sns.barplot(data=aux1, x='assortment', y='sales');\n",
    "\n",
    "# será que o assortment 'extra' era melhor e agora piorou?\n",
    "aux2 = df4[['year_week', 'assortment', 'sales']].groupby(['year_week', 'assortment']).sum().reset_index()\n",
    "aux2.pivot(index='year_week', columns='assortment', values='sales').plot() # transforma os assortments em colunas e os 'year_week' em linhas\n",
    "\n",
    "# como será o comportamento do assortment 'extra', dado que no gráfico anterior não pudemos enxergá-lo com clareza por causa da escala?\n",
    "aux3 = aux2[aux2['assortment'] == 'extra']\n",
    "aux3.pivot(index='year_week', columns='assortment', values='sales').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Lojas com competidores mais próximos deveriam vender menos.\n",
    "**Resposta**: falsa. Lojas com competidores MAIS PRÓXIMOS vendem MAIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,3,1)\n",
    "aux1 = df4[['competition_distance', 'sales']].groupby('competition_distance').sum().reset_index()\n",
    "#sns.barplot(data=aux1, x='competition_distance', y='sales'); # gráfico ficou muito granular\n",
    "sns.scatterplot(data=aux1, x='competition_distance', y='sales')\n",
    "\n",
    "# E se agrupássemos as distãncias por grupos de 1000 (bins)? Isso reduziria a granularidade do gráfico.\n",
    "plt.subplot(1,3,2)\n",
    "bins = list( np.arange(0, 20000, 1000) ) #limites arbitrários dos grupos\n",
    "aux1['competition_distance_binned'] = pd.cut( aux1['competition_distance'], bins=bins)\n",
    "aux2 = aux1[['competition_distance_binned', 'sales']].groupby('competition_distance_binned').sum().reset_index()\n",
    "sns.barplot(data=aux2, x='competition_distance_binned', y='sales');\n",
    "plt.xticks(rotation=90) #labels deste gráfico estavam sobrepostos. Vou rotacioná-los 90 graus.\n",
    "\n",
    "# força da correlação de pearson - usaremos um heatmap\n",
    "plt.subplot(1,3,3)\n",
    "sns.heatmap(aux1.corr(method='pearson'), annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Lojas com competidores há mais tempo vendem mais.\n",
    "**Resposta:** FALSA. Lojas com competidores há MAIS TEMPO vendem MENOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,3,1)\n",
    "aux1 = df4[['competition_time_month', 'sales']].groupby('competition_time_month').sum().reset_index()\n",
    "aux2 = aux1[ (aux1['competition_time_month'] < 120) & (aux1['competition_time_month'] != 0) ] # queremos competidores que não abriram no mesmo mês e têm menos de 120 meses.\n",
    "sns.barplot(data=aux2, x='competition_time_month', y='sales');\n",
    "plt.xticks(rotation=90);\n",
    "\n",
    "# 'competition_time_month' < 0 significa que o competidor VAI ABRIR depois da data em que a loja Rossman foi registrada no sistema\n",
    "# insight: quanto mais recente a competição, mais vendas a loja Rossman faz.\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "sns.regplot(data=aux2, x='competition_time_month', y='sales');\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "sns.heatmap(aux1.corr(method='pearson'), annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Lojas com promoções ativas por mais tempo deveriam vender mais.\n",
    "**Resposta:** FALSA. Elas vendem mais por determinado período após o início da promoção e, após, vendem menos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'promo_time_week' < 0 significa que a promoção estendida (promo2) VAI INICIAR no futuro.\n",
    "# Ou seja, a promo estendida está mais próxima quanto mais próximo de zero. Abaixo de zero ela VAI começar. Acima de zero ela JÁ COMEÇOU.\n",
    "aux1 = df4[['promo_time_week', 'sales']].groupby('promo_time_week').sum().reset_index()\n",
    "aux2 = aux1[aux1['promo_time_week'] > 0] # está na promo estendida\n",
    "aux3 = aux1[aux1['promo_time_week'] < 0] # está na promo regular\n",
    "\n",
    "grid = GridSpec(2,3) # cria uma grid com 2 linhas e 3 colunas\n",
    "\n",
    "plt.subplot(grid[0,0])\n",
    "sns.barplot(data=aux2, x='promo_time_week', y='sales');\n",
    "plt.xticks(rotation=90);\n",
    "\n",
    "plt.subplot(grid[0,1])\n",
    "sns.regplot(data=aux2, x='promo_time_week', y='sales'); # gráfico de regressão p/confirmar a tendência\n",
    "\n",
    "plt.subplot(grid[1,0])\n",
    "sns.barplot(data=aux3, x='promo_time_week', y='sales');\n",
    "plt.xticks(rotation=90);\n",
    "\n",
    "plt.subplot(grid[1,1])\n",
    "sns.regplot(data=aux3, x='promo_time_week', y='sales');\n",
    "\n",
    "plt.subplot(grid[:,2])\n",
    "sns.heatmap(aux1.corr(method='pearson'), annot=True); # correlação - mostra se a feature é relevante p/o modelo (corr. forte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Lojas com mais dias de promoção deveriam vender mais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resposta:** vamos validar no próximo ciclo do CRISP, já que ela é bem parecida com a anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Lojas com mais promoções consecutivas deveriam vender mais.\n",
    "**Resposta:** FALSA. Lojas com promoção estendida (promo2) venderam menos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# será que lojas que entraram na promoção estendida (promo2) venderam mais?\n",
    "df4[['promo', 'promo2', 'sales']].groupby(['promo', 'promo2']).sum().sort_values('sales', ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# como foi a performance ao longo do tempo das lojas que estavam nas duas promos?\n",
    "aux1 = df4[ (df4['promo'] == 1) & (df4['promo2'] == 1) ][['year_week', 'sales']].groupby('year_week').sum().reset_index()\n",
    "ax = aux1.plot()\n",
    "\n",
    "# como foi a performance ao longo do tempo das lojas que estavam apenas na promo?\n",
    "aux2 = df4[ (df4['promo'] == 1) & (df4['promo2'] == 0) ][['year_week', 'sales']].groupby('year_week').sum().reset_index()\n",
    "aux2.plot(ax=ax)\n",
    "\n",
    "ax.legend(labels=['Tradicional & Estendida', 'Tradicional'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Lojas deveriam vender menos durante os feriados escolares.\n",
    "**Resposta:** FALSA. As lojas vendem menos nos feriados escolares ao considerar o total de vendas. Mas existem muito mais dias que não são feriados escolares. Ao ignorar a diferença de tamanho dos dois grupos, considerando a MÉDIA DIÁRIA DE VENDAS dos dois grupos, verificamos que as lojas vendem mais durante feriados escolares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "aux = df4[['school_holiday', 'sales']].groupby('school_holiday').sum().reset_index()\n",
    "sns.barplot(data=aux, x='school_holiday', y='sales');\n",
    "\n",
    "# mas existem muito mais dias que não são school_holiday. Logo, o total de vendas\n",
    "# será muito maior no grupo \"não é school_holiday\"\n",
    "# um modo de ignorar o tamanho dos grupos é comparar a MÉDIA de vendas diárias de cada grupo\n",
    "plt.subplot(1,2,2)\n",
    "aux = df4[['school_holiday', 'sales']].groupby('school_holiday').mean().reset_index()\n",
    "aux = aux.rename(columns={'sales':'average_sales'})\n",
    "sns.barplot(data=aux, x='school_holiday', y='average_sales');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Lojas deveriam vender mais no segundo semestre.\n",
    "**Resposta:** FALSA. As lojas vendem menos no segundo semestre, a partir do mês 07."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,3,1)\n",
    "aux = df4[['month', 'sales']].groupby('month').sum().reset_index()\n",
    "sns.barplot(data=aux, x='month', y='sales');\n",
    "\n",
    "# tendência\n",
    "plt.subplot(1,3,2)\n",
    "sns.regplot(data=aux, x='month', y='sales');\n",
    "\n",
    "# correlação\n",
    "plt.subplot(1,3,3)\n",
    "sns.heatmap(aux.corr(method='pearson'), annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 09. Lojas deveriam vender menos aos finais de semana.\n",
    "**Resposta:** VERDADEIRA. As lojas vendem menos aos sábados e domingos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,3,1)\n",
    "aux = df4[['day_of_week', 'sales']].groupby('day_of_week').sum().reset_index()\n",
    "sns.barplot(data=aux, x='day_of_week', y='sales');\n",
    "\n",
    "# tendência\n",
    "plt.subplot(1,3,2)\n",
    "sns.regplot(data=aux, x='day_of_week', y='sales');\n",
    "\n",
    "# correlação\n",
    "plt.subplot(1,3,3)\n",
    "sns.heatmap(aux.corr(method='pearson'), annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Lojas abertas durante o feriado de Natal deveriam vender mais.\n",
    "**Resposta:** FALSA. Não vendem mais durante o Natal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "aux = df4[df4['state_holiday'] != 'regular_day']\n",
    "aux1 = aux[['state_holiday', 'sales']].groupby('state_holiday').sum().reset_index()\n",
    "sns.barplot(data=aux1, x='state_holiday', y='sales');\n",
    "\n",
    "# vendas nos feriados ao longo do tempo\n",
    "plt.subplot(1,2,2)\n",
    "aux2 = aux[['year', 'state_holiday', 'sales']].groupby(['year' ,'state_holiday']).sum().reset_index()\n",
    "sns.barplot(data=aux2, x='year', y='sales',hue='state_holiday');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Lojas deveriam vender mais ao longo dos anos.\n",
    "**Resposta:** FALSA. As vendas estão caindo aos longos dos anos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,3,1)\n",
    "aux = df4[['year', 'sales']].groupby('year').sum().reset_index()\n",
    "sns.barplot(data=aux, x='year', y='sales');\n",
    "\n",
    "# tendência\n",
    "plt.subplot(1,3,2)\n",
    "sns.regplot(data=aux, x='year', y='sales');\n",
    "\n",
    "# correlação\n",
    "plt.subplot(1,3,3)\n",
    "sns.heatmap(aux.corr(method='pearson'), annot=True);\n",
    "\n",
    "# tendência e correlação são tão fortes porque 2015 (o último ano) não está completo.\n",
    "# sem o ano 2015 tendência e correlação ficam fracas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Lojas deveriam vender mais depois do dia 10.\n",
    "**Resposta:** FALSA. Ao somar todos os 20 dias depois do dia 10, as lojas realmente vendem mais que nos 10 primeiros dias. Porém, ao considerar a média de vendas diárias em cada grupo (antes do dia 10 e depois do dia 10), verificamos que as lojas vendem menos depois do dia 10. É necessário usar a média diária de vendas para equilibrar a comparação entre 02 grupos de tamanhos distintos (antes e depois do dia 10). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2,3,1)\n",
    "aux = df4[['day', 'sales']].groupby('day').sum().reset_index()\n",
    "sns.barplot(data=aux, x='day', y='sales');\n",
    "\n",
    "# tendência\n",
    "plt.subplot(2,3,2)\n",
    "sns.regplot(data=aux, x='day', y='sales');\n",
    "\n",
    "# correlação\n",
    "plt.subplot(2,3,3)\n",
    "sns.heatmap(aux.corr(method='pearson'), annot=True);\n",
    "\n",
    "# o correto é dividir o dataset em 02 períodos: até o dia 10 e depois do dia 10\n",
    "# e então comparar as vendas destes 02 períodos.\n",
    "plt.subplot(2,3,4)\n",
    "aux['before_after'] = aux['day'].apply(lambda x: 'before_day_10' if x <= 10 else 'after_day_10')\n",
    "aux1 = aux[['before_after', 'sales']].groupby('before_after').sum().reset_index()\n",
    "sns.barplot(data=aux1, x='before_after', y='sales');\n",
    "\n",
    "# mas existem MAIS DIAS depois do dia 10 que dias antes do dia 10\n",
    "# logo, é provável que o grupo after somará mais que o grupo before\n",
    "# o correto seria calcular as vendas médias por dia por grupo (before e after)\n",
    "plt.subplot(2,3,5)\n",
    "aux2 = aux[['before_after', 'sales']].groupby('before_after').mean().reset_index()\n",
    "aux2 = aux2.rename(columns={'sales':'average_sales'})\n",
    "sns.barplot(data=aux2, x='before_after', y='average_sales');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1. Resumo das Hipóteses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = [ ['Hipoteses', 'Conclusao', 'Relevancia'], \n",
    "        ['H1', 'Falsa', 'Média'],\n",
    "        ['H2', 'Falsa', 'Alta'],\n",
    "        ['H3', 'Falsa', 'Alta'],\n",
    "        ['H4', 'Falsa', 'Média'],\n",
    "        ['H5', '-', '-'],\n",
    "        ['H6', 'Falsa', 'Alta'],\n",
    "        ['H7', 'Falsa', 'Baixa'],\n",
    "        ['H8', 'Falsa', 'Alta'],\n",
    "        ['H9', 'Verdadeira', 'Alta'],\n",
    "        ['H10', 'Falsa', 'Alta'],\n",
    "        ['H11', 'Falsa', 'Baixa'],\n",
    "        ['H12', 'Falsa', 'Média']\n",
    "]\n",
    "\n",
    "print (tabulate(tab, headers='firstrow'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Análise Multivariada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1. Numerical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = num_attributes.corr(method='pearson')\n",
    "sns.heatmap(correlation, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2. Categorical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apenas dados categóricos\n",
    "cat_attributes = df4.select_dtypes(include='object')\n",
    "\n",
    "# calculando Cramer V\n",
    "a1 = cramer_v(cat_attributes['state_holiday'], cat_attributes['state_holiday'])\n",
    "a2 = cramer_v(cat_attributes['state_holiday'], cat_attributes['store_type'])\n",
    "a3 = cramer_v(cat_attributes['state_holiday'], cat_attributes['assortment'])\n",
    "\n",
    "a4 = cramer_v(cat_attributes['store_type'], cat_attributes['state_holiday'])\n",
    "a5 = cramer_v(cat_attributes['store_type'], cat_attributes['store_type'])\n",
    "a6 = cramer_v(cat_attributes['store_type'], cat_attributes['assortment'])\n",
    "\n",
    "a7 = cramer_v(cat_attributes['assortment'], cat_attributes['state_holiday'])\n",
    "a8 = cramer_v(cat_attributes['assortment'], cat_attributes['store_type'])\n",
    "a9 = cramer_v(cat_attributes['assortment'], cat_attributes['assortment'])\n",
    "\n",
    "# dataset final\n",
    "d = pd.DataFrame({\n",
    "    'state_holiday':[a1, a2, a3],\n",
    "    'store_type':[a4,a5,a6],\n",
    "    'assortment':[a7,a8,a9]\n",
    "})\n",
    "\n",
    "d = d.set_index(d.columns)\n",
    "sns.heatmap(d, annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Preparação dos Dados\n",
    "\n",
    "Procure variáveis com distribuições normais no passo 4.1.2 que tem os histogramas das variáveis numéricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Normalização\n",
    "\n",
    "Da análise das variáveis numéricas, não encontramos nenhuma com distribuição normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df5.select_dtypes(include=['int64', 'float64'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se tem outlier: use robust-scaler. Se não, use min-max scaler\n",
    "rs = RobustScaler()\n",
    "mms = MinMaxScaler()\n",
    "path = '/media/joaomarcos/Ubuntu-data/repos/DS-em-producao/rossman_sales_prediction/parameters'\n",
    "\n",
    "# competition_distance\n",
    "df5['competition_distance'] = rs.fit_transform( df5[['competition_distance']].values )\n",
    "pickle.dump(rs, open(path + '/competition_distance_scaler.pkl', 'wb'))\n",
    "\n",
    "# year\n",
    "df5['year'] = mms.fit_transform( df5[['year']].values )\n",
    "pickle.dump(mms, open(path + '/year_scaler.pkl', 'wb'))\n",
    "\n",
    "# competition_time_month\n",
    "df5['competition_time_month'] = rs.fit_transform( df5[['competition_time_month']].values )\n",
    "pickle.dump(rs, open(path + '/competition_time_month_scaler.pkl', 'wb'))\n",
    "\n",
    "# promo_time_week\n",
    "df5['promo_time_week'] = mms.fit_transform( df5[['promo_time_week']].values )\n",
    "pickle.dump(rs, open(path + '/promo_time_week_scaler.pkl', 'wb'))\n",
    "\n",
    "# sns.boxplot(x=df5['year']); para ver se a distribuição tem outliers\n",
    "#sns.displot(df5['competition_distance'], kind='kde', height=7, aspect=2); # para ver o resultado após rescaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Transformação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1. Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_holiday - One Hot Encoding\n",
    "# é uma variável de estados: é feriado ou não é feriado\n",
    "df5 = pd.get_dummies(df5, prefix=['state_holiday'], columns=['state_holiday'])\n",
    "\n",
    "# store_type - Label Encoding\n",
    "# não tem uma ordem entre elas\n",
    "le = LabelEncoder()\n",
    "df5['store_type'] = le.fit_transform(df5['store_type'])\n",
    "pickle.dump(le, open(path + '/store_type_scaler.pkl', 'wb'))\n",
    "\n",
    "# assortment - Ordinal Encoding\n",
    "# existe uma ordem entre as variáveis: lojas com mais ou menos assortment\n",
    "assortment_dict = {'basic': 1, 'extra': 2, 'extended': 3}\n",
    "df5['assortment'] = df5['assortment'].map(assortment_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2. Transformação da Variável Resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5['sales'] = np.log1p(df5['sales'])\n",
    "sns.displot(data = df5['sales']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3. Transformação de Natureza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variaveis com natureza cíclica:\n",
    "# vamos implementar Transformação Seno-Cosseno\n",
    "\n",
    "# month\n",
    "df5['month_sin'] = df5['month'].apply(lambda x: np.sin( x * (2 * np.pi/12) ) )\n",
    "df5['month_cos'] = df5['month'].apply(lambda x: np.cos( x * (2 * np.pi/12) ) )\n",
    "\n",
    "# day\n",
    "df5['day_sin'] = df5['day'].apply(lambda x: np.sin( x * (2 * np.pi/30) ) )\n",
    "df5['day_cos'] = df5['day'].apply(lambda x: np.cos( x * (2 * np.pi/30) ) )\n",
    "\n",
    "# week_of_year\n",
    "df5['week_of_year_sin'] = df5['week_of_year'].apply(lambda x: np.sin( x * (2 * np.pi/52) ) )\n",
    "df5['week_of_year_cos'] = df5['week_of_year'].apply(lambda x: np.cos( x * (2 * np.pi/52) ) )\n",
    "\n",
    "# day_of_week\n",
    "df5['day_of_week_sin'] = df5['day_of_week'].apply(lambda x: np.sin( x * (2 * np.pi/7) ) )\n",
    "df5['day_of_week_cos'] = df5['day_of_week'].apply(lambda x: np.cos( x * (2 * np.pi/7) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.0. Seleção de Atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = df5.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_drop = ['week_of_year', 'day', 'month', 'day_of_week', 'promo_since', 'competition_since', 'year_week']\n",
    "df6 = df6.drop(cols_drop, axis=1)\n",
    "\n",
    "# saving and importing to not need to execute entire notebook\n",
    "path = '/media/joaomarcos/Ubuntu-data/repos/DS-em-producao/rossman_sales_prediction/df6.csv'\n",
    "#df6.to_csv(path, date_format='%Y-%m-%d')   \n",
    "df6 = pd.read_csv(path)\n",
    "df6['date'] = pd.to_datetime(df6['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Dividindo o dataframe em dataset de treino e de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as últimas 06 semanas do dataframe serão teste\n",
    "# o período anterior será usado para treino\n",
    "\n",
    "# primeira data do conjunto de treino - 02-01-2013\n",
    "#df6[['store', 'date']].groupby('store').min().reset_index()\n",
    "\n",
    "# última data do dataset - 31-07-2015\n",
    "#df6[['store', 'date']].groupby('store').max().reset_index()\n",
    "\n",
    "# qual a data 06 semanas antes da última do dataset? É 19-06-2015\n",
    "# ou seja, de 02-01-2013 até 19-06-2015 é nosso conjunto de treino\n",
    "df6[['store', 'date']].groupby('store').max().reset_index()['date'][0] - datetime.timedelta(days=6*7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training dataset\n",
    "X_train = df6[df6['date'] < '2015-06-19']\n",
    "y_train = X_train['sales']\n",
    "\n",
    "# testing dataset\n",
    "X_test = df6[df6['date'] >= '2015-06-19']\n",
    "y_test = X_test['sales']\n",
    "\n",
    "# print('Training Min Date:{}'.format(x_train['date'].min()))\n",
    "# print('Training Max Date:{}'.format(x_train['date'].max()))\n",
    "\n",
    "# print('\\nTesting Min Date:{}'.format(x_test['date'].min()))\n",
    "# print('Testing Max Date:{}'.format(x_test['date'].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Boruta como seletor de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and testing dataset for Boruta\n",
    "X_train_n = X_train.drop(['date', 'sales'], axis=1).values\n",
    "y_train_n = y_train.values.ravel()\n",
    "\n",
    "# define RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_jobs=-2, max_depth=6) \n",
    "# reduced max_depth and core usage to reduce execution time\n",
    "# it took ~ 03 hours on Google Colab TPU\n",
    "\n",
    "# define Boruta - it may take 5-30 hours and use all available memory\n",
    "# original version:\n",
    "# rf = RandomForestRegressor(n_jobs=-1)\n",
    "# boruta = BorutaPy(rf, n_estimators='auto', verbose=2, random_state=42).fit(X_train_n, y_train_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1. Best Features from Boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols_selected = boruta.support_.tolist()\n",
    "\n",
    "# # best features\n",
    "# X_train_fs = X_train.drop(['date', 'sales'], axis=1)\n",
    "# cols_selected_boruta = X_train_fs.iloc[:, cols_selected].columns.to_list()\n",
    "\n",
    "# # features not selected by Boruta\n",
    "# cols_not_selected_boruta = list(np.setdiff1d(X_train_fs.columns, cols_selected_boruta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_selected_boruta = [\n",
    "    'store',\n",
    "    'promo',\n",
    "    'store_type',\n",
    "    'assortment',\n",
    "    'competition_distance',\n",
    "    'competition_open_since_month',\n",
    "    'competition_open_since_year',\n",
    "    'promo2',\n",
    "    'promo2_since_week',\n",
    "    'promo2_since_year',\n",
    "    'competition_time_month',\n",
    "    'promo_time_week',\n",
    "    'day_of_week_sin',\n",
    "    'day_of_week_cos',\n",
    "    'month_sin',\n",
    "    'month_cos',\n",
    "    'day_sin',\n",
    "    'day_cos',\n",
    "    'week_of_year_sin',\n",
    "    'week_of_year_cos'\n",
    "]\n",
    "\n",
    "# columns to add\n",
    "feat_to_add = ['date', 'sales']\n",
    "\n",
    "cols_selected_boruta_full = cols_selected_boruta.copy()\n",
    "cols_selected_boruta_full.extend(feat_to_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare as variáveis selecionadas pelo Boruta com aquelas que você atribuiu relevância alta na análise exploratória. Se houver contradição, use a sugestão do Boruta neste primeiro ciclo de projeto. Use sua intuição para mesclar a sua seleção com a do Boruta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.0. Modelos de Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X_train[cols_selected_boruta] # only relevant classes\n",
    "x_test = X_test[cols_selected_boruta]\n",
    "\n",
    "# Time Series Data Preparation\n",
    "cols_selected_boruta.extend(feat_to_add)\n",
    "\n",
    "x_training = X_train[cols_selected_boruta_full] # relevant classes + date and sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Modelo de Média"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CÓDIGO QUE EU ACHO CORRETO. O CORRETO É PASSAR O TRAIN PARA O MODELO APRENDER, NÃO O TEST.\n",
    "\n",
    "# creates training dataset with sales figures\n",
    "train = x_train.copy() # training dataset without sales values\n",
    "train['sales'] = y_train.copy() # training series with sales values\n",
    "\n",
    "# testing dataset without sales values\n",
    "test = x_test.copy()\n",
    "test_sales = y_test.copy() \n",
    "\n",
    "# sales average per store on the training period\n",
    "avg_sales_train = train[['store', 'sales']].groupby('store').mean().reset_index().rename(columns={'sales':'predictions'}) \n",
    "\n",
    "# forecast\n",
    "forecast = pd.merge(test, avg_sales_train, how='left', on='store')\n",
    "baseline = forecast['predictions'] # sales forecast series\n",
    "\n",
    "# performance\n",
    "baseline_result1 = ml_error('Average Model 1', np.expm1(test_sales), np.expm1(baseline)) # used exponential function because parameters were in log format\n",
    "baseline_result1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Modelo de Regressão Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "lr = LinearRegression().fit(x_train, y_train)\n",
    "\n",
    "# prediction\n",
    "yhat_lr = lr.predict(x_test)\n",
    "\n",
    "# performance\n",
    "lr_result = ml_error('Linear Regression', np.expm1(y_test), np.expm1(yhat_lr))\n",
    "lr_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.1. Linear Regression Model - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_result_cv = cross_validation(x_training, 5, 'Linear Regression', lr)\n",
    "lr_result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3. Modelo de Linear Regression Regularized - Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "lrr = Lasso(alpha=0.01).fit(x_train, y_train)\n",
    "\n",
    "# prediction\n",
    "yhat_lrr = lrr.predict(x_test)\n",
    "\n",
    "# performance\n",
    "lrr_result = ml_error('Linear Regression - Lasso', np.expm1(y_test), np.expm1(yhat_lrr))\n",
    "lrr_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.1. Lasso Linear Regression - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrr_result_cv = cross_validation(x_training, 5, 'Lasso', lrr)\n",
    "lrr_result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4. Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model\n",
    "# rf = RandomForestRegressor(n_estimators=100, n_jobs=-2, random_state=42, max_depth=6).fit(x_train, y_train)\n",
    "\n",
    "# # prediction\n",
    "# yhat_rf = rf.predict(x_test)\n",
    "\n",
    "# # performance\n",
    "# rf_result = ml_error('Random Forest Regressor', np.expm1(y_test), np.expm1(yhat_rf))\n",
    "# rf_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.1. Random Forest Regressor - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_result_cv = cross_validation(x_training, 5, 'Random Forest', rf, verbose=True)\n",
    "# rf_result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model\n",
    "# model_xgb = xgb.XGBRegressor(max_depth=6).fit(x_train, y_train)\n",
    "\n",
    "# # prediction\n",
    "# yhat_xgb = model_xgb.predict(x_test)\n",
    "\n",
    "# # performance\n",
    "# xgb_result = ml_error('XGBoost Regressor', np.expm1(y_test), np.expm1(yhat_xgb))\n",
    "# xgb_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5.1. XGBoost Regressor - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_result_cv = cross_validation(x_training, 5, 'XGBoost', model_xgb)\n",
    "# xgb_result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6. Real Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelling_results_cv = pd.concat([lr_result_cv, lrr_result_cv, rf_result_cv, xgb_result_cv])\n",
    "# modelling_results_cv.sort_values('RMSE Cross-Validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Hyperparameter Fine Tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.1. Random Search - Implementação real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param = {\n",
    "#         'n_estimators': [100, 150, 200, 250, 300], \n",
    "#         'eta': [0.01, 0.03],\n",
    "#         'max_depth': [3, 5, 9],\n",
    "#         'subsample': [0.1, 0.5, 0.7],\n",
    "#         'colsample_bytree': [0.3, 0.7, 0.9],\n",
    "#         'min_child_weight': [3, 8, 15]\n",
    "#         }\n",
    "\n",
    "# MAX_EVAL = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_result = pd.DataFrame()\n",
    "\n",
    "# for i in range(MAX_EVAL):\n",
    "#     # escolha os valores dos parâmetros aleatoriamente\n",
    "#     hp = { k: random.sample(v, 1)[0] for k, v in param.items() }\n",
    "#     print(hp)\n",
    "\n",
    "#     # model\n",
    "#     model_xgb = xgb.XGBRegressor(\n",
    "#                                 objective='reg:squarederror',\n",
    "#                                 n_estimators=hp['n_estimators'],\n",
    "#                                 eta=hp['eta'],\n",
    "#                                 max_depth=hp['max_depth'],\n",
    "#                                 subsample=hp['subsample'],\n",
    "#                                 colsample_bytree=hp['colsample_bytree'],\n",
    "#                                 min_child_weight=hp['min_child_weight']\n",
    "#                                 )\n",
    "\n",
    "#     # performance\n",
    "#     result = cross_validation(x_training, 5, 'XGBoost Regressor', model_xgb)\n",
    "#     final_result = pd.concat([final_result, result])\n",
    "\n",
    "# final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Modelo Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recebe os parâmetros de melhor desempenho descobertos pelo Random Search\n",
    "param_tuned = {\n",
    "        'n_estimators': 300, \n",
    "        'eta': 0.03,\n",
    "        'max_depth': 9,\n",
    "        'subsample': 0.5,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'min_child_weight': 3\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model\n",
    "# model_xgb_tuned = xgb.XGBRegressor(\n",
    "#                             objective='reg:squarederror',\n",
    "#                             n_estimators=param_tuned['n_estimators'],\n",
    "#                             eta=param_tuned['eta'],\n",
    "#                             max_depth=param_tuned['max_depth'],\n",
    "#                             subsample=param_tuned['subsample'],\n",
    "#                             colsample_bytree=param_tuned['colsample_bytree'],\n",
    "#                             min_child_weight=param_tuned['min_child_weight']\n",
    "#                             ).fit(x_train, y_train)\n",
    "\n",
    "# # prediction\n",
    "# yhat_xgb_tuned = model_xgb_tuned.predict(x_test)\n",
    "\n",
    "# # performance\n",
    "# result = ml_error('XGBoost Regressor', np.expm1(y_test), np.expm1(yhat_xgb_tuned))\n",
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.1. Salvando o modelo treinado para não precisar treinar novamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/media/joaomarcos/Ubuntu-data/repos/DS-em-producao/rossman_sales_prediction//model/model_rossman.pkl'\n",
    "\n",
    "# saving model\n",
    "#pickle.dump(model_xgb_tuned, open(path, 'wb'))\n",
    "\n",
    "# loading model\n",
    "model_xgb_tuned = pickle.load(open(path, 'rb'))\n",
    "\n",
    "# prediction\n",
    "yhat_xgb_tuned = model_xgb_tuned.predict(x_test)\n",
    "\n",
    "# performance\n",
    "result = ml_error('XGBoost Regressor', np.expm1(y_test), np.expm1(yhat_xgb_tuned))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9 = X_test[cols_selected_boruta_full]\n",
    "\n",
    "# rescale\n",
    "df9['sales'] = np.expm1(df9['sales'])\n",
    "df9['predictions'] = np.expm1(yhat_xgb_tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1. Business Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of predictions\n",
    "df9_sum_predictions = df9[['store', 'predictions']].groupby('store').sum().reset_index()\n",
    "\n",
    "# MAE and MAPE for every store\n",
    "df9_mae = df9[['store', 'sales', 'predictions']].groupby('store').apply(lambda x: mean_absolute_error(x['sales'], x['predictions'])).reset_index().rename(columns={0:'MAE'})\n",
    "df9_mape = df9[['store', 'sales', 'predictions']].groupby('store').apply(lambda x: mean_absolute_percentage_error(x['sales'], x['predictions'])).reset_index().rename(columns={0:'MAPE'})\n",
    "\n",
    "# merge\n",
    "#df9_errors = pd.merge(df9_mae, df9_mape, how='inner', on='store')\n",
    "#df9_pred_compare = pd.merge(df9_sum_predictions, df9_errors, how='inner', on='store')\n",
    "df9_pred_compare = pd.concat([df9_sum_predictions, df9_mae, df9_mape], axis=1,join='inner')\n",
    "df9_pred_compare = df9_pred_compare.T.drop_duplicates().T\n",
    "\n",
    "# scenarios\n",
    "df9_pred_compare['worst_scenario'] = df9_pred_compare['predictions'] - df9_pred_compare['MAE']\n",
    "df9_pred_compare['best_scenario'] = df9_pred_compare['predictions'] + df9_pred_compare['MAE']\n",
    "\n",
    "# formatting and ordering columns\n",
    "df9_pred_compare['store'] = df9_pred_compare['store'].astype('int64')\n",
    "df9_pred_compare[['predictions', 'worst_scenario', 'best_scenario', 'MAE', 'MAPE']] = df9_pred_compare[['predictions', 'worst_scenario', 'best_scenario', 'MAE', 'MAPE']].round(2)\n",
    "df9_pred_compare = df9_pred_compare[['store', 'predictions', 'worst_scenario', 'best_scenario', 'MAE', 'MAPE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most difficult stores to forecast sales\n",
    "print(df9_pred_compare.sort_values('MAPE', ascending=False).head(10))\n",
    "sns.scatterplot(x='store', y='MAPE', data=df9_pred_compare);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2. Total Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9_pred_compare[['predictions', 'worst_scenario', 'best_scenario']].sum().map('R${:,.2f}'.format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3. Machine Learning Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9['error'] = df9['sales']-df9['predictions']\n",
    "df9['error_rate'] = df9['predictions'] / df9['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2,2,1)\n",
    "sns.lineplot(x='date', y='sales', data=df9, label='SALES')\n",
    "sns.lineplot(x='date', y='predictions', data=df9, label='PREDICTIONS')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "sns.lineplot(x='date', y='error_rate', data=df9)\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "#sns.displot(data=df9, x = 'error', kde=True)\n",
    "#sns.distplot(df9['error'])\n",
    "sns.histplot(df9['error'], kde=True)\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "sns.scatterplot(x='predictions', y='error', data=df9);"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "32375aeba9747de29273227a337c285ee4172bae59ce6853782332c520fe4955"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('venv_DS-em-producao': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
